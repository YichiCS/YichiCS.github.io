
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yichi Zhang's homepage</title>
  
  <meta name="author" content="Yichi Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css?v=1.0.0">
  <!-- <link rel="icon" type="image/x-ico" href="/images/favicon.ico">
  <link rel="shortcut icon" type="image/x-ico" href="/images/favicon.ico">
  <link rel="apple-touch-icon" type="image/x-ico" href="/images/favicon.ico"> -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='90%' font-size='90'>üöÄ</text></svg>">
  <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='90%' font-size='90'>üöÄ</text></svg>">
  <link rel="apple-touch-icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='90%' font-size='90'>üöÄ</text></svg>">

  <style>
    .over {
      cursor: pointer;
    }
    a {
	  color: #8C1515;
	  background-color: transparent;
	  text-decoration: none;
	}
  .rp{
    text-indent: 1em;
  }
  
  .abs {
    cursor: pointer;
  }

  .question-mark {
      cursor: pointer;
      font-weight: bold;
      color: #8C1515; /* Change the color as needed */
      font-size: 100%; /* Adjust the font size to make it smaller */
      vertical-align: top; /* Display the question mark as a superscript */
    }

    .tooltip {
      display: none;
      position: absolute;
      background-color: #f9f9f9;
      padding: 5px;
      border: 1px solid #ccc;
      border-radius: 4px;
      box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
      font-size: 80%;
    }

  </style>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7QYG7WK9K4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7QYG7WK9K4');
</script>

<!-- Default Statcounter code for personal website https://chen-geng.com/ -->
<script type="text/javascript">
  var sc_project=12835460; 
  var sc_invisible=1; 
  var sc_security="d2e3b553"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="free hit counter"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12835460/0/d2e3b553/1/" alt="free hit counter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <!-- <name>Chen Geng „ÄåËÄø Êô®„Äç</name> -->
                <name>Yichi ZHANG</name>
                <cname>„ÄåÂº†‰∫¶Âºõ
                </cname>
                <span>
                  <span class="question-mark">?</span>
                  <span class="tooltip">
                    My first name is Chen, and my last name is Geng.
                    <br>I prefer to be addressed by my first name Chen.
                    <br>Possible pronunciation: Chen(ch-uhn) Geng(guh-ng).
                  </span>
                </span>
                <cname>„Äç
                </cname>
              </p>
              <p>
                Hi üëã! I am a final-year undergraduate student majoring in Information Engineering at <a href="https://www.zju.edu.cn/">Zhejiang University</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=5HoF_9oAAAAJ">Shouling Ji</a> and Prof. <a href="https://scholar.google.com/citations?user=a2WEREUAAAAJ">Qianqian Yang</a>. Starting in September 2025, I will join <a href="https://www.psu.edu/">Pennsylvania State University</a> as a Ph.D. student in Informatics under the supervision of Prof. <a href="https://scholar.google.com/citations?user=bugfOowAAAAJ">Yuchen Yang</a>. My current research interests primarily lie in <em><a>trustworthy machine learning</a></em> and <em><a>machine learning theory</a></em>.

        <div style="display:none" id="research_desc">
        <em>
        Not for now.</em> 
        <br><br>
        </div>

        This website is still under construction. I am learning how to use HTML and CSS to build a website instead of using a template...

    <!-- Previously, I got my bachelor's degree in Computer Science from <a href=./pages/zjucs.html>Zhejiang University</a> in 2023, with an honors degree from <a href=http://ckc.zju.edu.cn/ckcen/wbout/list.htm>Chu Kochen Honors College</a>. During my undergraduate, I was privileged to work closely with <a href="http://xzhou.me/">Prof. Xiaowei Zhou</a> and <a href="https://pengsida.net/">Prof. Sida Peng</a> on several research projects. I spent a wonderful summer at Stanford, also with the <a href="https://jiajunwu.com/">CogAI group</a>, in 2022. -->
    <br><br>

		<!-- If you happen to find any shared research interests, or if you just want to chat on any topics, especially if you are from underrepresented groups, don't hesitate to shoot me an email. I'm always up for potential collaborations.  -->
    If you have shared research interests or have any topics you'd like to chat about ‚Äî especially if you're from underrepresented groups ‚Äî don't hesitate to shoot me an email. I'm always up for exploring potential collaborations and/or engaging in insightful conversations.

		<br><br>

                <div align="center">
                  Email: X √ó Y, where 
                  X = {yichics02}, Y = {@gmail.com}
                  X = {yichizhang}, Y = {@zju.edu.cn}
                </div>
              </p>
              <!-- <p style="text-align:center"> -->
                <!-- <a href="mailto:gengchen@zju.edu.cn">Email</a> &nbsp/&nbsp -->
                <!-- <a href="files/resume0401.pdf?v=1.0.0">R√©sum√©</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?user=Q36tl7oAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/gengchen01">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/gcgeng">GitHub</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chen-geng-aa8a09125/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/chen_portrait.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/chen_circle3.png" class="hoverZoomLink"></a>
            </td> -->
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Recent travel ‚úàÔ∏è</heading>
            <p>
              <ul>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/09/2023 - 06/12/2023: VALSE 2023, Wuxi, China</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/16/2023 - 06/22/2023: CVPR 2023, Vancouver, Canada</li>
              </ul>
            </p>
          </td>
        </tr>
      </tbody>
      </table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Recent News üì∞</heading>
            <p>
              <ul>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2025: We will be organizing the workshop on "Generating Digital Twins from Images and Videos" at ICCV 2025.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2025: Our paper "Birth and Death of a Rose" is selected as oral presentation at CVPR 2025.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2025: One paper is (conditionally) accepted to SIGGRAPH 2025.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2025: Two papers are accepted to CVPR 2025.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2024: One paper is accepted to CVPR 2024.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2024: One paper is accepted to ICLR 2024.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2024: This year I will co-organize Stanford Graphics Cafe (Lunch Seminar). Subscribe to our mailing list <a href="https://mailman.stanford.edu/mailman/listinfo/graphics-cafe">here</a>!</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2023: One paper is accepted to ICCV 2023.</li>
                <!-- <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2023: I'll be joining Stanford University as a CS PhD student.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: One paper is accepted to CVPR 2023.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: One paper is accepted to TPAMI.</li>
                <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2022: One paper is accepted to SIGGRAPH 2022.</li> -->
              </ul>
            </p>
          </td>
        </tr>
      </tbody>
      </table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;padding-bottom: 2px;">
              <heading>Selected Research üî¨</heading>
              <p class="rp">
              <!-- For the comprehensive list, please refer to my <a href="files/resume0401.pdf?v=1.0.0">r√©sum√©</a>. -->
                <!-- <br> -->
                <!-- (* indicates equal contribution) -->
              </p>
            </td>
          </tr>
        </tbody></table>
        &nbsp&nbsp&nbsp&nbsp (* denotes equal contribution, ^ denotes student (co-)mentored, representative works are <span style="background-color: #ffffd0">highlighted</span>)
        <br><br>
         &nbsp&nbsp&nbsp&nbsp   For the comprehensive list, check out my <a href="https://scholar.google.com/citations?user=Q36tl7oAAAAJ&hl=en">Google Scholar</a> page.
         <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <br>

          <tr style="background-color: #ffffd0;">
            <td style="width:40%;vertical-align:middle;">
                <img src="media/teaser_canor.gif" width=100% height=auto style="bject-fit:cover">
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Category-Agnostic Neural Object Rigging</papertitle>
              <br>
              <a href="https://www.guangzhaohe.com/">Guangzhao He^*</a>,
              <strong>Chen Geng*</strong>,
              <a href="https://elliottwu.com/">Shangzhe Wu</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>
              <br>
              <em>CVPR 2025</em>
              <br>
              <a class="abs" href="/index.html">[To appear]</a>
              <p></p>
              <p>
                tl;dr: We discover animatable motion subspaces for any 4D objects.
              </p>
              <div style="background-color:#ffffd0">
            </div>
            </td>
          </tr>


          <!-- <tr> -->
          <tr style="background-color: #ffffd0;">
            <td style="width:40%;vertical-align:middle">
                <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="/rose4d/static/videos/teaser-small.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                <!-- <img src="media/relightable_avatar.mp4" width=100% height=auto style="bject-fit:cover"> -->
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Birth and Death of a Rose</papertitle>
              <br>
              <strong>Chen Geng</strong>,
              <a href="https://cs.stanford.edu/~yzzhang">Yunzhi Zhang</a>,
              <a href="https://elliottwu.com/">Shangzhe Wu</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>
              <br>
              <em>CVPR 2025 </em> 
              <br>
              <strong><a>(Oral Presentation, 3.3% of the accepted papers)</a></strong>
              <br>
              <a class="abs" href="/files/rose4d.pdf">[Paper]</a>
              <a class="abs" href="/rose4d/">[Project Page]</a>
              <a class="abs" href="https://github.com/gcgeng/rose4d/">[Code (Coming Soon)]</a>
              <a class="abs" href="https://arxiv.org/abs/2412.05278">[arXiv]</a>
              <p></p>
              <p>
                tl;dr: We generate temporal 4D object intrinsics from 2D foundation models.
              </p>
              <div style="background-color:#ffffd0">
            </div>
            </td>
          </tr>

          <tr>
            <script type="text/javascript">
              function ist_abs_sw() {
                old = document.getElementById('ist-abs').style.display
                console.log(old)
                if(old == "none") {
                  document.getElementById('ist-abs').style.display = "block"
                } else {
                  document.getElementById('ist-abs').style.display = "none"
                }
              }
              document.getElementById('ist-abs').style.display = "none"
            </script>
            <td style="width:40%;vertical-align:middle">
                <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="media/relightable_avatar.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
                <!-- <img src="media/relightable_avatar.mp4" width=100% height=auto style="bject-fit:cover"> -->
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Relightable and Animatable Neural Avatar from Sparse-View Video</papertitle>
              <br>
              <a href="https://zhenx.me/">Zhen Xu</a>,
              <a href="https://pengsida.net/">Sida Peng</a>,
              <strong>Chen Geng</strong>,
              <a href="https://scholar.google.com/citations?user=cIXq7Z4AAAAJ">Linzhan Mou</a>,
              <a href="https://www.media.mit.edu/people/yzihan/overview/">Zihan Yan</a>,
              <a href="https://jiamingsun.ml/">Jiaming Sun</a>,
              <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>,
              <a href="https://xzhou.me/">Xiaowei Zhou</a>
              <br>
              <em>CVPR 2024</em> 
              <br>
              <strong><a>(Highlight, 11.9% of the accepted papers)</a></strong>
              <br>
              <a class="abs" href="/files/xu_relighting.pdf">[Paper]</a>
              <a class="abs" href="https://zju3dv.github.io/relightable_avatar">[Project Page]</a>
              <a class="abs" href="https://github.com/zju3dv/RelightableAvatar">[Code]</a>
              <a class="abs" href="https://arxiv.org/abs/2308.07903">[arXiv]</a>
              <a class="abs" href="https://youtu.be/BQ3pL7Uwbdk">[Video]</a>
              <p></p>
              <p>
                tl;dr: We estimate physically based intrinsics of dynamic characters from monocular videos.
              </p>
              <div style="background-color:#ffffd0">
            </div>
            </td>
          </tr>

          <!-- <tr> -->
          <tr style="background-color: #ffffd0;">
            <script type="text/javascript">
              function ist_abs_sw() {
                old = document.getElementById('ist-abs').style.display
                console.log(old)
                if(old == "none") {
                  document.getElementById('ist-abs').style.display = "block"
                } else {
                  document.getElementById('ist-abs').style.display = "none"
                }
              }
              document.getElementById('ist-abs').style.display = "none"
            </script>
            <td style="width:40%;vertical-align:middle">
                <!-- <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="media/phasepgf_demo.gif" type="video/mp4">
                Your browser does not support the video tag.
                </video> -->
                <img src="media/phasepgf_demo.gif" width=100% height=auto style="bject-fit:cover">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Neural Polynomial Gabor Fields for Macro Motion Analysis</papertitle>
              <br>
              <strong>Chen Geng*</strong>,
              <a href="https://kovenyu.com/">Hong-Xing "Koven" Yu*</a>,
              <a href="https://pengsida.net/">Sida Peng</a>,
              <a href="https://xzhou.me/">Xiaowei Zhou</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>
              <br>
              <em>ICLR 2024</em>
              <br>
              <a class="abs" href="https://chen-geng.com/files/phasepgf.pdf">[Paper]</a>
              <a class="abs" href="https://chen-geng.com/phasepgf">[Project Page]</a>
              <a class="abs" href="https://github.com/gcgeng/phasepgf">[Code (Coming Soon)]</a>
              <a class="abs" href="https://openreview.net/forum?id=dTlKCQuuxP">[OpenReview]</a>
              <p></p>
              <p>
                tl;dr: We discover a low-dimensional interpretable motion representation for dynamic scenes with macro motion.
              </p>
              <div style="background-color:#ffffd0">
            </div>
            </td>
          </tr>
					
          <tr>
            <script type="text/javascript">
              function ist_abs_sw() {
                old = document.getElementById('ist-abs').style.display
                console.log(old)
                if(old == "none") {
                  document.getElementById('ist-abs').style.display = "block"
                } else {
                  document.getElementById('ist-abs').style.display = "none"
                }
              }
              document.getElementById('ist-abs').style.display = "none"
            </script>
            <td style="width:40%;vertical-align:middle">
                <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="media/inv-shade-trees.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Tree-Structured Shading Decomposition</papertitle>
              <br>
              <strong>Chen Geng*</strong>,
              <a href="https://kovenyu.com/">Hong-Xing "Koven" Yu*</a>,
              <a href="https://sxzhang25.github.io/">Sharon Zhang</a>,
              <a href="http://graphics.stanford.edu/~maneesh/">Maneesh Agrawala</a>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>
              <br>
              <em>ICCV 2023</em>
              <br>
              <a class="abs" href="/files/inv-shade-trees.pdf">[Paper]</a>
              <a class="abs" href="/inv-shade-trees/index.html">[Project Page]</a>
              <a class="abs" href="https://www.youtube.com/watch?v=L7zD9zM_zcg">[Video]</a>
              <a class="abs" href="https://github.com/gcgeng/inv-shade-trees">[Code]</a>
              <a class="abs" href="https://www.marktechpost.com/2023/10/05/researchers-at-stanford-present-a-novel-artificial-intelligence-method-that-can-effectively-and-efficiently-decompose-shading-into-a-tree-structured-representation/">[MarkTechPost]</a>
              <!-- <a class="abs" href="https://www.marktechpost.com/2023/10/05/researchers-at-stanford-present-a-novel-artificial-intelligence-method-that-can-effectively-and-efficiently-decompose-shading-into-a-tree-structured-representation/">[Media Coverage]</a> -->
              <p></p>
              <p>
                tl;dr: We decompose the shading of objects into a tree-structured representation, which can be edited or interpreted by users easily.
              </p>
              <div style="background-color:#ffffd0">
              <div style="display:none" id="ist-abs">
                <strong>Abstract:</strong> We study the problem of obtaining a tree-structured representation for shading objects. Prior work typically uses the parametric or measured representation to model shading, which is neither interpretable nor easily editable. Our method uses the shade tree representation, which combines basic shading nodes and compositing methods, to model and decomposes the material shading. Such a representation enables users to edit previously rigid material appearances in an efficient and intuitive manner. In particular, novice users who are unfamiliar with the construction of such shade trees can quickly obtain such a representation. The extraction of such a representation enables the editing and understanding of object shading, even for novice users.  The biggest challenge in this task is that the discrete structure of the shade tree is not differentiable. We propose a hybrid algorithm to address this issue. First, given an input image, a recursive amortized inference model is leveraged to initialize a guess of the tree structure and corresponding leaf node parameters. Then, we apply an optimization-based method to fine-tune the result. Experiments show that our method works well on synthetic images, realistic images, and non-realistic vector drawings, surpassing the baselines significantly.
              </div>
            </div>
            </td>
          </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
            <script type="text/javascript">
              function inb_abs_sw() {
                old = document.getElementById('inb-abs').style.display
                console.log(old)
                if(old == "none") {
                  document.getElementById('inb-abs').style.display = "block"
                } else {
                  document.getElementById('inb-abs').style.display = "none"
                }
              }
              document.getElementById('inb-abs').style.display = "none"
            </script>
            <td style="width:40%;vertical-align:middle">
                <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="media/inb.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learning Neural Volumetric Representations of Dynamic Humans in Minutes</papertitle>
              <br>
              <strong>Chen Geng*</strong>,
              <a href="https://pengsida.net/">Sida Peng*</a>,
              <a href="https://zhenx.me/">Zhen Xu*</a>,
              <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>,
              <a href="http://xzhou.me/">Xiaowei Zhou</a>
              <br>
              <em>CVPR 2023</em>
              <br>
              <a href="https://chen-geng.com/files/instant_nvr_cvpr.pdf">[Paper]</a>
              <a href="https://zju3dv.github.io/instant_nvr">[Project Page]</a>
              <a href="https://github.com/zju3dv/instant-nvr">[Code]</a>
              <!-- <a href="https://chen-geng.com/">project page</a> -->
              <!-- / -->
              <!-- <a href="https://arxiv.org/abs/2209.14988">arXiv</a> -->
              <!-- / -->
              <!-- <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a> -->
              <p>
                tl;dr: We accelerate the learning of neural volumetric videos of dynamic humans by over 100 times.
              </p>
              <p>
              </p>
              <div style="background-color:#ffffd0">
              <div style="display:none" id="inb-abs">
                <strong>Abstract:</strong>  This paper addresses the challenge of quickly reconstructing free-viewpoint videos of dynamic humans from sparse multi-view videos. Some recent works represent the dynamic human as a canonical neural radiance field (NeRF) and a motion field, which are learned from videos through differentiable rendering. They generally require a lengthy optimization process. Other generalization methods leverage learned prior from datasets and reduce the optimization time by only finetuning on new scenes, at the cost of visual fidelity. In this paper, we propose a novel method for creating viewpoint-free human performance synthesis from sparse view videos in minutes with competitive visual quality. Specifically, we leverage the human body prior to define a novel part-based voxelized NeRF representation, which distributes the representational power of the canonical human model efficiently. Furthermore, we propose a novel dimensionality reduction 2D motion parameterization scheme to increase the convergence rate of the human deformation field. Experiments demonstrate that our approach can be trained 100 times faster than prior per-scene optimiztion methods while being competitive in the rendering quality. We show that given a  video capturing a human performer of 100 frames, our model typically takes about 5 minutes for training to produce photorealistic free-viewpoint videos on a single RTX 3090 GPU. The code will be released for reproducibility.
              </div>
            </div>
            </td>
          </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
            <script type="text/javascript">
              function mnb_abs_sw() {
                old = document.getElementById('mnb-abs').style.display
                console.log(old)
                if(old == "none") {
                  document.getElementById('mnb-abs').style.display = "block"
                } else {
                  document.getElementById('mnb-abs').style.display = "none"
                }
              }
              document.getElementById('mnb-abs').style.display = "none"
            </script>
            <td style="width:40%;vertical-align:middle">
                <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="https://chen-geng.com/media/neuralbody.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Implicit Neural Representations with Structured Latent Codes for Human Body Modeling</papertitle>
              <br>
              <a href="https://pengsida.net/">Sida Peng</a>,
              <strong>Chen Geng</strong>,
              <a href='https://yuanqing-zhang.github.io/'>Yuanqing Zhang</a>,
              <a href="https://justimyhxu.github.io/">Yinghao Xu</a>,
              <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
              <a href="https://chingswy.github.io/">Qing Shuai</a>,
              <a href="http://xzhou.me/">Xiaowei Zhou</a>,
              <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>
              <br>
              <em>TPAMI 2023</em>
              <br>
              <a href="https://chen-geng.com/files/nbpami2.pdf">[Paper]</a>
              <a href="https://github.com/zju3dv/neuralbody">[Code]</a>
              <a href="https://ieeexplore.ieee.org/document/10045794">[IEEE Xplore]</a>
              <!-- <a onclick="mnb_abs_sw()">[Bibtex]</a> -->
              <!-- <a href="http://github.com/zju3dv/neuralbody">[Code]</a> -->
              <!-- <a href="https://chingswy.github.io/easymocap-public-doc/works/multinb.html">[Project Page]</a> -->
              <!-- <a href="https://chen-geng.com/">project page</a> -->
              <!-- / -->
              <!-- <a href="https://arxiv.org/abs/2209.14988">arXiv</a> -->
              <!-- / -->
              <!-- <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a> -->
              <p>
                tl;dr: Our approach reconstruct geometry and appearance of human performers with high accuracy from sparse observations.
              </p>
              <p>
              </p>
              <!-- <div style="background-color:#ffffd0">
              <div style="display:none" id="mnb-abs">
                @inproceedings{multinb,
                  <br>
                      &nbsp&nbsp&nbsp&nbsp title = {Novel View Synthesis of Human Interactions from Sparse
                  Multi-view Videos},
                  <br>
                      &nbsp&nbsp&nbsp&nbsp author = {Qing, Shuai and Chen, Geng and Qi, Fang and Sida, Peng and Wenhao, Shen and Xiaowei, Zhou and Hujun, Bao},
                  <br>
                      &nbsp&nbsp&nbsp&nbsp booktitle = {SIGGRAPH Conference Proceedings},
                  <br>
                      &nbsp&nbsp&nbsp&nbsp year = {2022},
                  <br>
                }
              </div>
              </div> -->
            </td>
          </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
            <script type="text/javascript">
              function mnb_abs_sw() {
                old = document.getElementById('mnb-abs').style.display
                console.log(old)
                if(old == "none") {
                  document.getElementById('mnb-abs').style.display = "block"
                } else {
                  document.getElementById('mnb-abs').style.display = "none"
                }
              }
              document.getElementById('mnb-abs').style.display = "none"
            </script>
            <td style="width:40%;vertical-align:middle">
                <video  width=100% height=auto style="bject-fit:cover" muted autoplay loop playsinline>
                <source src="media/mnb2.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Novel View Synthesis of Human Interactions from Sparse Multi-view Videos</papertitle>
              <br>
              <a href="https://chingswy.github.io/">Qing Shuai</a>,
              <strong>Chen Geng</strong>,
              <a href='https://raypine.github.io/'>Qi Fang</a>,
              <a href="https://pengsida.net/">Sida Peng</a>,
              <a href="http://shenwenhao01.github.io/">Wenhao Shen</a>,
              <a href="http://xzhou.me/">Xiaowei Zhou</a>,
              <a href="http://www.cad.zju.edu.cn/home/bao/">Hujun Bao</a>
              <br>
              <em>SIGGRAPH 2022 </em>
              <br>
              <strong><a>(Featured in the <a href="https://youtu.be/sK2EZE617pY?si=lvam-QNnv4OJBNS-&t=56">technical paper trailer</a>)</a></strong>
              <br>
              <a href="https://chen-geng.com/files/mnb.pdf">[Paper]</a>
              <a onclick="mnb_abs_sw()">[Bibtex]</a>
              <a href="https://github.com/zju3dv/EasyMocap">[Code]</a>
              <a href="https://chingswy.github.io/easymocap-public-doc/works/multinb.html">[Project Page]</a>
              <!-- <a href="https://chen-geng.com/">project page</a> -->
              <!-- / -->
              <!-- <a href="https://arxiv.org/abs/2209.14988">arXiv</a> -->
              <!-- / -->
              <!-- <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a> -->
              <p>
                tl;dr: Given sparse multi-view videos of crowded scenes with multiple human performers, our approach is able to generate high-fidelity novel views and accurate instance masks. 
              </p>
              <p>
              </p>
              <div style="background-color:#ffffd0">
              <div style="display:none" id="mnb-abs">
                @inproceedings{multinb,
                  <br>
                      &nbsp&nbsp&nbsp&nbsp title = {Novel View Synthesis of Human Interactions from Sparse
                  Multi-view Videos},
                  <br>
                      &nbsp&nbsp&nbsp&nbsp author = {Qing, Shuai and Chen, Geng and Qi, Fang and Sida, Peng and Wenhao, Shen and Xiaowei, Zhou and Hujun, Bao},
                  <br>
                      &nbsp&nbsp&nbsp&nbsp booktitle = {SIGGRAPH Conference Proceedings},
                  <br>
                      &nbsp&nbsp&nbsp&nbsp year = {2022},
                  <br>
                }
              </div>
            </div>
            </td>
          </tr>


        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <heading>Experience üßë‚Äçüéì</heading>
              </td>
            </tr>
          </tbody>
        </table>
        <table width="100%" align="center" border="0" cellpadding="10">
          <tbody>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                  src="images/stanford.png" , width="100%"></td>
              <td width="80%" valign="center">
                <b>Stanford University</b>
                <br> 2023 - Present, Stanford, California <br>
                <br> <b>PhD Student</b>
                <br> Advisor: Prof. <a href="https://jiajunwu.com/">Jiajun Wu</a>
                <!-- <br> Awards: <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2024-north-america">Finalist, Qualcomm Innovation Fellowship 2024</a> -->
              </td>
            </tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                 src="images/zju3.png" , width="100%"></td>
              <td width="80%" valign="center">
                <b>Zhejiang University</b>
                <br> 2019 - 2023, Hangzhou, China <br>
                <br> <b>B.Eng.(Honours) in Computer Science</b>
                <br> Cumulative GPA: 94.38/100, 3.99/4.0
                <br> Major GPA: 96.67/100, 4.0/4.0
                <br> Advisor: Prof. <a href="http://xzhou.me/">Xiaowei Zhou</a>
              </td>
            </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
              <td>
                <heading>Professional Activities üèõÔ∏è</heading>
              </td>
            </tr>
          </tbody>
        </table>
        <table width="100%" align="center" border="0" cellpadding="10">
          <tbody>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                  src="images/professional_activities.png" , width="100%"></td>
              <td width="80%" valign="center">
              <b>Conference Reviewer</b>: CVPR (2024-), ECCV (2024-), ICCV (2025-), SIGGRAPH (2025), SIGGRAPH Asia (2024), ICML (2024-), ICLR (2025-), NeurIPS (2024-), NeurIPS D&B (2024-), ICRA (2024), 3DV (2023), AAAI (2024), Pacific Graphics (2024).
              <br>
              <br>
              <b>Journal Reviewer</b>: <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (T-PAMI), <em>IEEE Transactions on Visualization and Computer Graphics</em> (TVCG).
              <br>
              <br>
              <b>Co-organizer</b>: Stanford GCaf√© (Graphics Lunch Seminar) 2024, <a href=https://3dvconf.github.io/2025/people>3DV 2025</a>.
              <br>
              <br>
              <b>Mentorship and Outreach</b>: Stanford PhD Application Support Program for Underrepresented Group (SASP) 2023, Stanford CS Undergraduate Mentoring Program 2023.
              <br>
              <br>
              <b>Misc.</b>: <a href=https://www.qualcomm.com/research/university-relations/innovation-fellowship/finalists>Finalist, Qualcomm Innovation Fellowship, 2024</a>.
              </td>
            </tr>
          </tbody>
        </table>

        <br><br>

				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
        <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=n&d=Rn5MtIIxM2RRMgAGbjy50xJP7Li7QUPHkh7uoUNKqlM'></script> -->
              <br>
              <p style="text-align:right;">
                This website is adapted from <a href="https://jonbarron.info/">this awesome template</a>
                <br>
                Last updated: Apr. 2025
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<script async src="//static.getclicky.com/101390101.js"></script>
<script>
  const questionMark = document.querySelector('.question-mark');
  const tooltip = document.querySelector('.tooltip');

  questionMark.addEventListener('mouseover', () => {
    tooltip.style.display = 'inline-block';
  });

  questionMark.addEventListener('mouseout', () => {
    tooltip.style.display = 'none';
  });

  // Optional: If you want to hide the tooltip when the mouse moves away from it, add the following code
  tooltip.addEventListener('mouseout', () => {
    tooltip.style.display = 'none';
  });
</script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101390101ns.gif" /></p></noscript>

<script defer src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon='{"rayId":"93860404af6e035b","version":"2025.4.0-1-g37f21b1","r":1,"token":"586fd614a75843bda8e1e371a8e27116","serverTiming":{"name":{"cfExtPri":true,"cfL4":true,"cfSpeedBrain":true,"cfCacheStatus":true}}}' crossorigin="anonymous"></script>
</body>

</html>
